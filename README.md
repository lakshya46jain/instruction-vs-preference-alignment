# Instruction-vs-Preference Alignment

Supervised Fine-Tuning (SFT) + Preference Alignment (DPO/RLHF)

This repository contains the code, datasets, and training pipelines for our CS 4804 group project exploring **instruction tuning** vs. **preference-based alignment** techniques using modern LLMs.

Our workflow follows the real-world alignment stack:

1. **Pretraining** (pretrained open-source model â€” not done here)
2. **Supervised Fine-Tuning (SFT)**
3. **Preference Optimization (DPO / RLHF)**
4. **Evaluation & Analysis**

This repository includes all assets necessary for the SFT stage and downstream alignment tasks.

---

## Project Datasets

We work with three datasets:

| Filename               | Purpose                                                       | Use in Project |
| ---------------------- | ------------------------------------------------------------- | -------------- |
| `alpaca_data.json`     | Pre-cleaned instruction-tuning dataset (general instructions) | âœ” Used         |
| `code_alpaca_20k.json` | Code instruction dataset (raw)                                | âœ˜ Not used     |
| `cleaned_data.json`    | Cleaned version of code dataset                               | âœ” Used         |

For this project, **only two datasets are used for fine-tuning**:

- `alpaca_data.json`
- `cleaned_data.json`

These are combined, formatted into a unified prompt style, and split into train/validation sets.

---

## ðŸ“ Repository Structure

```
group_project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ alpaca_data.json
â”‚   â”‚   â”œâ”€â”€ cleaned_data.json
â”‚   â”‚   â””â”€â”€ code_alpaca_20k.json (not used)
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ sft_train.jsonl
â”‚       â””â”€â”€ sft_val.jsonl
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ prepare_sft_data.py      # Generates formatted JSONL datasets
â”‚
â”œâ”€â”€ sft/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py               # Tokenize + mask labels + dataset loader
â”‚   â”œâ”€â”€ sft_config.py            # Model + training configuration
â”‚   â”œâ”€â”€ train_sft_mac.py         # LoRA SFT for Apple Silicon (MPS)
â”‚   â”œâ”€â”€ train_sft_cuda.py        # QLoRA SFT for CUDA GPUs
â”‚   â””â”€â”€ eval_sft.py              # (optional) Compare base vs SFT outputs
â”‚
â””â”€â”€ README.md
```

---

# What is Supervised Fine-Tuning (SFT)?

SFT is the first stage of alignment after pretraining.
The goal is to make the pretrained model **follow instructions** by training it on `(instruction, response)` pairs using supervised learning.

For each example:

```
### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}
```

The SFT model learns to:

- Incorporate the instruction
- Use optional inputs
- Produce the correct output
- Learn structured formatting
- Reason better than the base model

Later, the preference alignment team will start from the fine-tuned SFT model and apply:

- DPO (Direct Preference Optimization)
- RLHF-style training

Your SFT outputs are the foundation for the rest of the project.

---

## Recommended Python Version

Use **Python 3.11**.
Do **NOT** use Python 3.13 â€” it breaks Transformers, datasets, and PyTorch.

The simplest way is via **pyenv**:

```bash
brew install pyenv pyenv-virtualenv
echo 'eval "$(pyenv init -)"' >> ~/.zshrc
echo 'eval "$(pyenv virtualenv-init -)"' >> ~/.zshrc
source ~/.zshrc
```

Install Python:

```bash
pyenv install 3.11.8
pyenv virtualenv 3.11.8 sft-env
pyenv local sft-env
```

Verify:

```bash
python3 --version
# Python 3.11.8
```

---

## Install Dependencies

Inside the activated environment:

```bash
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install transformers datasets peft accelerate sentencepiece
```

---

# Step 1 â€” Prepare the SFT Dataset

Before running any training, generate formatted JSONL files using:

```bash
python scripts/prepare_sft_data.py
```

This produces:

- `data/processed/sft_train.jsonl`
- `data/processed/sft_val.jsonl`

---

# Step 2 â€” Run SFT on Mac (LoRA + MPS)

Apple Silicon **cannot** run QLoRA (no bitsandbytes support), so LoRA is used.

Run:

```bash
python -m sft.train_sft_mac
```

This will:

- Load model in full precision
- Apply LoRA adapters
- Use Appleâ€™s `mps` backend
- Train on the combined dataset

Outputs are saved in:

```
models/sft_output/
```

---

# Step 3 â€” Run SFT on CUDA GPU (QLoRA 4-bit)

If you have access to a GPU server or Google Colab with CUDA:

```bash
python -m sft.train_sft_cuda
```

This uses:

- 4-bit quantization via bitsandbytes
- QLoRA
- Much lower memory usage
- Faster training

Outputs are saved in the same directory:

```
models/sft_output/
```

---

# Step 4 â€” Evaluate SFT Model (Optional)

To compare base vs SFT outputs:

```bash
python -m sft.eval_sft
```

This script loads:

- Base model
- SFT LoRA adapter

and prints side-by-side generations.

---

# Training Notes

- All padding and label masking are handled in `dataset.py`
- All models use a shared configuration in `sft/sft_config.py`
- You can modify LoRA hyperparameters there
- For CUDA machines, `train_sft_cuda.py` mirrors `train_sft_mac.py` with quantization enabled

---

# Credits

This repository was developed as part of the **CS 4804 â€“ Introduction to Artificial Intelligence** group project at Virginia Tech.

The SFT implementation is based on:

- HuggingFace Transformers
- PEFT (LoRA / QLoRA)
- HuggingFace Datasets
- PyTorch (MPS / CUDA)

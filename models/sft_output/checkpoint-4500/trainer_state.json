{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.2776877863655297,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030854198485058856,
      "grad_norm": 47.45829391479492,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 11.551,
      "step": 50
    },
    {
      "epoch": 0.006170839697011771,
      "grad_norm": 15.39688491821289,
      "learning_rate": 3.960000000000001e-05,
      "loss": 6.2902,
      "step": 100
    },
    {
      "epoch": 0.009256259545517656,
      "grad_norm": 0.3179817795753479,
      "learning_rate": 5.96e-05,
      "loss": 0.4697,
      "step": 150
    },
    {
      "epoch": 0.012341679394023542,
      "grad_norm": 0.1880241334438324,
      "learning_rate": 7.960000000000001e-05,
      "loss": 0.1735,
      "step": 200
    },
    {
      "epoch": 0.015427099242529427,
      "grad_norm": 0.15772590041160583,
      "learning_rate": 9.960000000000001e-05,
      "loss": 0.1519,
      "step": 250
    },
    {
      "epoch": 0.018512519091035313,
      "grad_norm": 0.31872785091400146,
      "learning_rate": 0.00011960000000000001,
      "loss": 0.1373,
      "step": 300
    },
    {
      "epoch": 0.0215979389395412,
      "grad_norm": 0.2250533401966095,
      "learning_rate": 0.0001396,
      "loss": 0.1412,
      "step": 350
    },
    {
      "epoch": 0.024683358788047085,
      "grad_norm": 0.1642877459526062,
      "learning_rate": 0.0001596,
      "loss": 0.144,
      "step": 400
    },
    {
      "epoch": 0.027768778636552967,
      "grad_norm": 0.16440050303936005,
      "learning_rate": 0.0001796,
      "loss": 0.132,
      "step": 450
    },
    {
      "epoch": 0.030854198485058854,
      "grad_norm": 0.1502918303012848,
      "learning_rate": 0.0001996,
      "loss": 0.136,
      "step": 500
    },
    {
      "epoch": 0.03393961833356474,
      "grad_norm": 0.18198220431804657,
      "learning_rate": 0.00019782222222222224,
      "loss": 0.1432,
      "step": 550
    },
    {
      "epoch": 0.037025038182070626,
      "grad_norm": 0.20932582020759583,
      "learning_rate": 0.0001956,
      "loss": 0.1314,
      "step": 600
    },
    {
      "epoch": 0.04011045803057651,
      "grad_norm": 0.140708789229393,
      "learning_rate": 0.00019337777777777779,
      "loss": 0.1431,
      "step": 650
    },
    {
      "epoch": 0.0431958778790824,
      "grad_norm": 0.1348150074481964,
      "learning_rate": 0.00019115555555555556,
      "loss": 0.1516,
      "step": 700
    },
    {
      "epoch": 0.04628129772758828,
      "grad_norm": 0.14548546075820923,
      "learning_rate": 0.00018893333333333334,
      "loss": 0.1358,
      "step": 750
    },
    {
      "epoch": 0.04936671757609417,
      "grad_norm": 0.11274144798517227,
      "learning_rate": 0.00018671111111111114,
      "loss": 0.1527,
      "step": 800
    },
    {
      "epoch": 0.05245213742460005,
      "grad_norm": 0.14304180443286896,
      "learning_rate": 0.00018448888888888889,
      "loss": 0.1466,
      "step": 850
    },
    {
      "epoch": 0.055537557273105935,
      "grad_norm": 0.14321212470531464,
      "learning_rate": 0.0001822666666666667,
      "loss": 0.1406,
      "step": 900
    },
    {
      "epoch": 0.058622977121611825,
      "grad_norm": 0.15478961169719696,
      "learning_rate": 0.00018004444444444446,
      "loss": 0.1379,
      "step": 950
    },
    {
      "epoch": 0.06170839697011771,
      "grad_norm": 0.14141666889190674,
      "learning_rate": 0.0001778222222222222,
      "loss": 0.1394,
      "step": 1000
    },
    {
      "epoch": 0.06479381681862359,
      "grad_norm": 0.15088902413845062,
      "learning_rate": 0.0001756,
      "loss": 0.1431,
      "step": 1050
    },
    {
      "epoch": 0.06787923666712949,
      "grad_norm": 0.16742204129695892,
      "learning_rate": 0.0001733777777777778,
      "loss": 0.1346,
      "step": 1100
    },
    {
      "epoch": 0.07096465651563537,
      "grad_norm": 0.13573698699474335,
      "learning_rate": 0.00017115555555555556,
      "loss": 0.1378,
      "step": 1150
    },
    {
      "epoch": 0.07405007636414125,
      "grad_norm": 0.11962209641933441,
      "learning_rate": 0.00016893333333333334,
      "loss": 0.1442,
      "step": 1200
    },
    {
      "epoch": 0.07713549621264713,
      "grad_norm": 0.1290280669927597,
      "learning_rate": 0.00016671111111111114,
      "loss": 0.137,
      "step": 1250
    },
    {
      "epoch": 0.08022091606115302,
      "grad_norm": 0.11714024096727371,
      "learning_rate": 0.0001644888888888889,
      "loss": 0.1218,
      "step": 1300
    },
    {
      "epoch": 0.08330633590965891,
      "grad_norm": 0.12468061596155167,
      "learning_rate": 0.0001622666666666667,
      "loss": 0.1338,
      "step": 1350
    },
    {
      "epoch": 0.0863917557581648,
      "grad_norm": 0.17723208665847778,
      "learning_rate": 0.00016004444444444444,
      "loss": 0.1351,
      "step": 1400
    },
    {
      "epoch": 0.08947717560667068,
      "grad_norm": 0.12363320589065552,
      "learning_rate": 0.00015782222222222224,
      "loss": 0.1376,
      "step": 1450
    },
    {
      "epoch": 0.09256259545517656,
      "grad_norm": 0.15366970002651215,
      "learning_rate": 0.00015560000000000001,
      "loss": 0.1349,
      "step": 1500
    },
    {
      "epoch": 0.09564801530368244,
      "grad_norm": 0.14129117131233215,
      "learning_rate": 0.00015337777777777776,
      "loss": 0.1226,
      "step": 1550
    },
    {
      "epoch": 0.09873343515218834,
      "grad_norm": 0.12769009172916412,
      "learning_rate": 0.00015115555555555556,
      "loss": 0.1398,
      "step": 1600
    },
    {
      "epoch": 0.10181885500069422,
      "grad_norm": 0.14713788032531738,
      "learning_rate": 0.00014893333333333334,
      "loss": 0.1511,
      "step": 1650
    },
    {
      "epoch": 0.1049042748492001,
      "grad_norm": 0.1276707947254181,
      "learning_rate": 0.00014671111111111111,
      "loss": 0.1371,
      "step": 1700
    },
    {
      "epoch": 0.10798969469770599,
      "grad_norm": 0.12874959409236908,
      "learning_rate": 0.0001444888888888889,
      "loss": 0.1424,
      "step": 1750
    },
    {
      "epoch": 0.11107511454621187,
      "grad_norm": 0.14692853391170502,
      "learning_rate": 0.0001422666666666667,
      "loss": 0.1334,
      "step": 1800
    },
    {
      "epoch": 0.11416053439471777,
      "grad_norm": 0.11411340534687042,
      "learning_rate": 0.00014004444444444444,
      "loss": 0.1403,
      "step": 1850
    },
    {
      "epoch": 0.11724595424322365,
      "grad_norm": 0.11039610952138901,
      "learning_rate": 0.00013782222222222224,
      "loss": 0.1288,
      "step": 1900
    },
    {
      "epoch": 0.12033137409172953,
      "grad_norm": 0.12296485155820847,
      "learning_rate": 0.00013560000000000002,
      "loss": 0.134,
      "step": 1950
    },
    {
      "epoch": 0.12341679394023541,
      "grad_norm": 0.12137213349342346,
      "learning_rate": 0.0001333777777777778,
      "loss": 0.1258,
      "step": 2000
    },
    {
      "epoch": 0.1265022137887413,
      "grad_norm": 0.13748495280742645,
      "learning_rate": 0.00013115555555555557,
      "loss": 0.14,
      "step": 2050
    },
    {
      "epoch": 0.12958763363724718,
      "grad_norm": 0.11730526387691498,
      "learning_rate": 0.00012893333333333334,
      "loss": 0.1405,
      "step": 2100
    },
    {
      "epoch": 0.13267305348575306,
      "grad_norm": 0.10278022289276123,
      "learning_rate": 0.00012671111111111112,
      "loss": 0.1298,
      "step": 2150
    },
    {
      "epoch": 0.13575847333425897,
      "grad_norm": 0.11554019153118134,
      "learning_rate": 0.0001244888888888889,
      "loss": 0.137,
      "step": 2200
    },
    {
      "epoch": 0.13884389318276485,
      "grad_norm": 0.13935552537441254,
      "learning_rate": 0.00012226666666666667,
      "loss": 0.119,
      "step": 2250
    },
    {
      "epoch": 0.14192931303127074,
      "grad_norm": 0.11438138782978058,
      "learning_rate": 0.00012004444444444445,
      "loss": 0.1379,
      "step": 2300
    },
    {
      "epoch": 0.14501473287977662,
      "grad_norm": 0.13129444420337677,
      "learning_rate": 0.00011782222222222223,
      "loss": 0.1463,
      "step": 2350
    },
    {
      "epoch": 0.1481001527282825,
      "grad_norm": 0.1728362888097763,
      "learning_rate": 0.00011559999999999999,
      "loss": 0.1283,
      "step": 2400
    },
    {
      "epoch": 0.15118557257678839,
      "grad_norm": 0.1238875612616539,
      "learning_rate": 0.00011337777777777778,
      "loss": 0.1564,
      "step": 2450
    },
    {
      "epoch": 0.15427099242529427,
      "grad_norm": 0.14200079441070557,
      "learning_rate": 0.00011115555555555557,
      "loss": 0.1377,
      "step": 2500
    },
    {
      "epoch": 0.15735641227380015,
      "grad_norm": 0.1271354705095291,
      "learning_rate": 0.00010893333333333333,
      "loss": 0.1354,
      "step": 2550
    },
    {
      "epoch": 0.16044183212230603,
      "grad_norm": 0.14540323615074158,
      "learning_rate": 0.00010671111111111112,
      "loss": 0.1294,
      "step": 2600
    },
    {
      "epoch": 0.16352725197081192,
      "grad_norm": 0.11987798660993576,
      "learning_rate": 0.0001044888888888889,
      "loss": 0.1316,
      "step": 2650
    },
    {
      "epoch": 0.16661267181931783,
      "grad_norm": 0.16508056223392487,
      "learning_rate": 0.00010226666666666667,
      "loss": 0.1338,
      "step": 2700
    },
    {
      "epoch": 0.1696980916678237,
      "grad_norm": 0.1351754516363144,
      "learning_rate": 0.00010004444444444446,
      "loss": 0.1333,
      "step": 2750
    },
    {
      "epoch": 0.1727835115163296,
      "grad_norm": 0.13905489444732666,
      "learning_rate": 9.782222222222223e-05,
      "loss": 0.1456,
      "step": 2800
    },
    {
      "epoch": 0.17586893136483547,
      "grad_norm": 0.1463778018951416,
      "learning_rate": 9.56e-05,
      "loss": 0.1213,
      "step": 2850
    },
    {
      "epoch": 0.17895435121334136,
      "grad_norm": 0.13326360285282135,
      "learning_rate": 9.337777777777778e-05,
      "loss": 0.1216,
      "step": 2900
    },
    {
      "epoch": 0.18203977106184724,
      "grad_norm": 0.12994101643562317,
      "learning_rate": 9.115555555555556e-05,
      "loss": 0.1421,
      "step": 2950
    },
    {
      "epoch": 0.18512519091035312,
      "grad_norm": 0.12933729588985443,
      "learning_rate": 8.893333333333333e-05,
      "loss": 0.1446,
      "step": 3000
    },
    {
      "epoch": 0.188210610758859,
      "grad_norm": 0.09549761563539505,
      "learning_rate": 8.671111111111112e-05,
      "loss": 0.1278,
      "step": 3050
    },
    {
      "epoch": 0.1912960306073649,
      "grad_norm": 0.19167082011699677,
      "learning_rate": 8.44888888888889e-05,
      "loss": 0.141,
      "step": 3100
    },
    {
      "epoch": 0.19438145045587077,
      "grad_norm": 0.12049844861030579,
      "learning_rate": 8.226666666666667e-05,
      "loss": 0.1277,
      "step": 3150
    },
    {
      "epoch": 0.19746687030437668,
      "grad_norm": 0.10943438857793808,
      "learning_rate": 8.004444444444444e-05,
      "loss": 0.1302,
      "step": 3200
    },
    {
      "epoch": 0.20055229015288256,
      "grad_norm": 0.14625228941440582,
      "learning_rate": 7.782222222222223e-05,
      "loss": 0.131,
      "step": 3250
    },
    {
      "epoch": 0.20363771000138844,
      "grad_norm": 0.12750151753425598,
      "learning_rate": 7.560000000000001e-05,
      "loss": 0.1334,
      "step": 3300
    },
    {
      "epoch": 0.20672312984989433,
      "grad_norm": 0.11340391635894775,
      "learning_rate": 7.337777777777778e-05,
      "loss": 0.1401,
      "step": 3350
    },
    {
      "epoch": 0.2098085496984002,
      "grad_norm": 0.12468226253986359,
      "learning_rate": 7.115555555555556e-05,
      "loss": 0.1316,
      "step": 3400
    },
    {
      "epoch": 0.2128939695469061,
      "grad_norm": 0.15222425758838654,
      "learning_rate": 6.893333333333333e-05,
      "loss": 0.1287,
      "step": 3450
    },
    {
      "epoch": 0.21597938939541197,
      "grad_norm": 0.11286994069814682,
      "learning_rate": 6.671111111111111e-05,
      "loss": 0.13,
      "step": 3500
    },
    {
      "epoch": 0.21906480924391786,
      "grad_norm": 0.14672167599201202,
      "learning_rate": 6.448888888888888e-05,
      "loss": 0.1281,
      "step": 3550
    },
    {
      "epoch": 0.22215022909242374,
      "grad_norm": 0.13963152468204498,
      "learning_rate": 6.226666666666667e-05,
      "loss": 0.1217,
      "step": 3600
    },
    {
      "epoch": 0.22523564894092965,
      "grad_norm": 0.12128520756959915,
      "learning_rate": 6.0044444444444446e-05,
      "loss": 0.1394,
      "step": 3650
    },
    {
      "epoch": 0.22832106878943553,
      "grad_norm": 0.14001327753067017,
      "learning_rate": 5.782222222222222e-05,
      "loss": 0.1293,
      "step": 3700
    },
    {
      "epoch": 0.23140648863794142,
      "grad_norm": 0.12734918296337128,
      "learning_rate": 5.560000000000001e-05,
      "loss": 0.1401,
      "step": 3750
    },
    {
      "epoch": 0.2344919084864473,
      "grad_norm": 0.10316048562526703,
      "learning_rate": 5.3377777777777785e-05,
      "loss": 0.1355,
      "step": 3800
    },
    {
      "epoch": 0.23757732833495318,
      "grad_norm": 0.1285596638917923,
      "learning_rate": 5.115555555555556e-05,
      "loss": 0.1295,
      "step": 3850
    },
    {
      "epoch": 0.24066274818345906,
      "grad_norm": 0.13461938500404358,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.137,
      "step": 3900
    },
    {
      "epoch": 0.24374816803196495,
      "grad_norm": 0.16867223381996155,
      "learning_rate": 4.671111111111111e-05,
      "loss": 0.1362,
      "step": 3950
    },
    {
      "epoch": 0.24683358788047083,
      "grad_norm": 0.1514144390821457,
      "learning_rate": 4.448888888888889e-05,
      "loss": 0.1412,
      "step": 4000
    },
    {
      "epoch": 0.2499190077289767,
      "grad_norm": 0.11057071387767792,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.1235,
      "step": 4050
    },
    {
      "epoch": 0.2530044275774826,
      "grad_norm": 0.1371026486158371,
      "learning_rate": 4.004444444444445e-05,
      "loss": 0.1305,
      "step": 4100
    },
    {
      "epoch": 0.2560898474259885,
      "grad_norm": 0.12729665637016296,
      "learning_rate": 3.782222222222222e-05,
      "loss": 0.1364,
      "step": 4150
    },
    {
      "epoch": 0.25917526727449436,
      "grad_norm": 0.1302042007446289,
      "learning_rate": 3.56e-05,
      "loss": 0.1285,
      "step": 4200
    },
    {
      "epoch": 0.26226068712300027,
      "grad_norm": 0.12687090039253235,
      "learning_rate": 3.337777777777778e-05,
      "loss": 0.1265,
      "step": 4250
    },
    {
      "epoch": 0.2653461069715061,
      "grad_norm": 0.11054645478725433,
      "learning_rate": 3.1155555555555555e-05,
      "loss": 0.1177,
      "step": 4300
    },
    {
      "epoch": 0.26843152682001203,
      "grad_norm": 0.12755060195922852,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.1386,
      "step": 4350
    },
    {
      "epoch": 0.27151694666851794,
      "grad_norm": 0.12579970061779022,
      "learning_rate": 2.6711111111111115e-05,
      "loss": 0.1287,
      "step": 4400
    },
    {
      "epoch": 0.2746023665170238,
      "grad_norm": 0.1734859049320221,
      "learning_rate": 2.448888888888889e-05,
      "loss": 0.129,
      "step": 4450
    },
    {
      "epoch": 0.2776877863655297,
      "grad_norm": 0.10772673040628433,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.1371,
      "step": 4500
    }
  ],
  "logging_steps": 50,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.14533364400128e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
